# [Google IT Support Professional Certificate](https://www.coursera.org/professional-certificates/google-it-support) Notes

## [Course 1: Techhnical Support Fundamentals](https://www.coursera.org/learn/technical-support-fundamentals?specialization=google-it-support)
***

## Week1

- ## **Introduction to IT Support**

     <details><summary> What is IT? </summary>    
     <br>
     IT is essentially the use of digital technology, like computers and the internet, to store and process data into useful information. The IT industry refers to the entire scope of all the jobs and resources that are related to computing technologies within society, and there are a lot of different types of jobs in this field, from network engineers who ensure computers can communicate with each other, to hardware technicians who replace and repair components, to desktop support personnel who make sure that end users can use their software properly.   
     </details>

     <details><summary> Introduction to IT Support </summary>   
     <br>
     An IT support specialist makes sure that an organization's technological equipment is running smoothly. This includes managing, installing, maintaining, troubleshooting and configuring office and computing equipment.  
     </details>

- ## **History of Computing**

     <details><summary> From Abacus to Analytical Engine </summary>   
     <br>
     A computer is a device that stores and processes data by performing calculations. Before we had actual computer devices, the term computer was used to refer to someone who actually did the calculation.      
     <br><br>
     Do you know what an abacus is?
     <br>
     It looks like a wooden toy that a child would play with, but it's actually one of the earliest known computers. It was invented in 500 BC to count large numbers. While we have calculators like the old reliable TI-89s or the ones in our computers, abacuses actually are still used today.
     <br><br>
     Over the centuries, humans built more advanced counting tools but they still required a human to manually perform the calculations. The first major step forward was the invention of the mechanical calculator in the 17th century by Blaise Pascal. This device used a series of gears and levers to perform calculations for the user automatically. While it was limited to addition, subtraction, multiplication and division for pretty small numbers, it paved the way for more complex machines. 
     <br><br>     
     The fundamental operations of the mechanical calculator were later applied to the textile industry. Before we had streamlined manufacturing, looms were used to weave yarn into fabric. If you wanted to design patterns on your fabric, that took an incredible amount of manual work.
     <br>
     In the 1800s, a man by the name of Joseph Jacquard invented a programmable loom. These looms took a sequence of cards with holes in them. When the loom encountered a hole, it would hook the thread underneath it. If it didn't encounter a hole, the hook wouldn't thread anything. Eventually this spun up a design pattern on the fabric. These cards were known as punch cards. And while Mr. Jacquard reinvented the textile industry, he probably didn't realize that his invention would shaped the world of computing and the world itself today.
     <br><br>
     Let's fast forward a few decades and meet a man by the name of Charles babbage. Babbage was a gifted engineer who developed a series of machines that are now known as the greatest breakthrough on our way to the modern computer. He built what was called a difference engine. It was a very sophisticated version of some of the mechanical calculators we were just talking about. It could perform fairly complicated mathematical operations but not much else. Babbage's follow up to the difference engine was a machine he called the Analytical Engine. He was inspired by Jacquard's use of punch cards to automatically perform calculations instead of manually entering them by hand. Babbage used punch cards in his Analytical engine to allow people to predefine a series of calculations they wanted to perform. As impressive as this achievement was, the Analytical engine was still just a very advanced mechanical calculator.
     <br><br>
     It took the powerful insights of a mathematician named Ada Lovelace to realize the true potential of the analytical engine. She was the first person to recognize that the machine could be used for more than pure calculations. She developed the first algorithm for the engine. It was the very first example of computer programming. An algorithm is just a series of steps that solves specific problems. Because of Lovelace's discovery that algorithms could be programmed into the Analytical engine, it became the very first general purpose computing machine in history, and a great example that women have had some of the most valuable minds in technology since the 1800s.
     </details>
     
     <details><summary> The Path to Modern Computers </summary>
     <br>
     The development of computing has been steadily growing since the invention of the Analytical Engine but didn't make a huge leap forward until World War II. Back then, research into computing was super expensive, electronic components were large and you needed lots of them to compute anything of value. This also meant that computers took up a ton of space and many efforts were underfunded and unable to make headway. When the war broke out, governments started pouring money and resources into computing research. They wanted to help develop technologies that would give them advantages over other countries, lots of efforts were spun up and advancements were made in fields like cryptography. _Cryptography_ is the art of writing and solving codes. During the war, computers were used to process secret messages from enemies faster than a human could ever hope to do. 
     <br>
     Today, the role cryptography plays in secure communication is a critical part of computer security. Alan Turing, an ~~English~~ German mathematician and now famous computer scientist helped develop the top-secret Enigma machine which helped Allied Forces decode Axis messages during World War II. The Enigma machine is just one of the examples of how governments started to recognize the potential of computation. After the war, companies like IBM, Hewlett-Packard, and others were advancing their technologies into the academic, business, and government realms. Lots of technological advancements and computing were made in the 20th century thanks to direct interest from governments, scientists, and companies left over from World War II. These organizations invented new methods to store data in computers which fueled the growth of computational power. Consider this, until the 1950s punch cards were a popular way to store data. Operators would have decks of ordered punch cards that were used for data processing. If they dropped the deck by accident and the cards got out of order, it was almost impossible to get them sorted again. There were obviously some limitations to punch cards, but thanks to new technological innovations like magnetic tape and its counterparts, people began to store more data on more reliable media. A magnetic tape worked by magnetizing data onto a tape. Back in the 1970s and 80s, people used to listen to music on vinyl records or cassette tapes. These relics are examples of how magnetic tapes can store information and run that information from a machine. This left stacks and stacks of punch cards to collect dust while their new magnetic tape counterparts began to revolutionize the industry.
     <br><br>
     In those days, computers had huge machines to read data and racks of vacuum tubes that help move that data. Vacuum tubes control the electricity voltages and all sorts of electronic equipment like televisions and radios, but these specific vacuum tubes were bulky and broke all the time. Imagine what the work of an I.T. support specialist was like in those early days of computing. The job description might have included crawling around inside huge machines filled with dust and creepy crawly things, or replacing vacuum tubes and swapping out those punch cards. In those days, doing some debugging might have taken on a more literal meaning. Renowned computer scientist Admiral Grace Hopper had a favorite story involving some engineers working on the Harvard Mark II computer. They were trying to figure out the source of the problems in a relay. After doing some investigating, they discovered the source of their trouble was a moth, a literal bug in the computer. 
     <br>
     The ENIAC was one of the earliest forms of general purpose computers. It was a wall-to-wall convolution of massive electronic components and wires. It had 17,000 vacuum tubes and took up about 1,800 square feet of floor space. Imagine if you had to work with that scale of equipment today. I wouldn't want to share an office with 1,800 square feet of machinery. Eventually, the industry started using transistors to control electricity voltages. This is now a fundamental component of all electronic devices. Transistors perform almost the same functions as vacuum tubes but they are more compact and more efficient. You can easily have billions of transistors in a small computer chip today. Throughout the decades, more and more advancements were made. The very first compiler was invented by Admiral Grace Hopper. Compilers made it possible to translate human language via a programming language into machine code. Eventually, the industry gave way to the first hard disk drives and microprocessors. Then, programming language started becoming the predominant way for engineers to develop computer software. 
     <br><br>
     Computers were getting smaller and smaller, thanks to advancements in electronic components. Instead of filling up entire rooms like ENIAC, they were getting small enough to fit on tabletops. The Xerox Alto was the first computer that resembled the computers we're familiar with now. It was also the first computer to implement a graphical user interface that used icons, a mouse, and a window. Some of you may remember that the sheer size and cost of historical computers made it almost impossible for an average family to own one. Instead, they were usually found in military and university research facilities. When companies like Xerox started building machines at a relatively affordable price and at a smaller form factor, the consumer age of computing began. Then in the 1970s, a young engineer named Steve Wozniak invented the Apple I, a single-board computer MIT for hobbyists. With his friend Steve Jobs, they created a company called Apple Computer. Their follow up to the Apple I, the Apple II, was ready for the average consumer to use. The Apple II was a phenomenal success, selling for nearly two decades and giving a new generation of people access to personal computers. For the first time, computers became affordable for the middle class and helped bring computing technology into both the home and office. In the 1980s, IBM introduced its personal computer. It was released with a primitive version of an operating system called MS DOS or Microsoft Disk Operating System. Back to IBM's PC, it was widely adopted and made more accessible to consumers, thanks to a partnership with Microsoft. Microsoft, founded by Bill Gates, eventually created Microsoft Windows. For decades it was the preferred operating system in the workplace and dominated the computing industry because it could be run on any compatible hardware. With more computers in the workplace, the dependence on I.T. rose and so did the demand for skilled workers who could support that technology. Not only were personal computers entering the household for the first time, but a new type of computing was emerging: video games. 
     <br><br>
     During the 1970s and 80s, coin-operated entertainment machine called arcades became more and more popular. A company called Atari developed one of the first coin-operated arcade games in 1972 called Pong. Pong was such a sensation that people were standing in lines at bars and rec centers for hours at a time to play. Entertainment computers like Pong launch the video game era. Eventually, Atari went on to launch the video computer system which help bring personal video consoles into the home. Video games have contributed to the evolution of computers in a very real way, tell that to the next person who dismisses them as a toy. Video game show people that computers didn't always have to be all work and no play, they were a great source of entertainment too. This was an important milestone for the computing industry, since at that time, computers were primarily used in the workplace or at research institutions. 
     <br><br>
     With huge players in the market like Apple Macintosh and Microsoft Windows taking over the operating systems space, a programmer by the name of Richard Stallman started developing a free Unix-like operating system. Unix was an operating system developed by Ken Thompson and Dennis Ritchie, but it wasn't cheap and wasn't available to everyone. Stallman created an OS that he called GNU. It was meant to be free to use with similar functionality to Unix. Unlike Windows or Macintosh, GNU wasn't owned by a single company, its code was open source which meant that anyone could modify and share it. GNU didn't evolve into a full operating system, but it set a foundation for the formation of one of the largest open source operating system, Linux, which was created by Linus Torvalds.
     <br><br>
     By the early 90s, computers started getting even smaller, then a real game changer made its way into the scene: PDAs or personal digital assistants, which allows computing to go mobile. These mobile devices included portable media players, word processors, email clients, Internet browsers, and more all in one handy handheld device. In the late 1990s, Nokia introduced a PDA with mobile phone functionality. This ignited an industry of pocketable computers or as we know them today, smartphones.
     </details>
    
- ## **Digital Logic**

     <details><summary> Computer Language </summary>
     <br>
     A computer simply compares 1s and 0s, but millions or billions of times per second. The communication that a computer uses is referred to as binary system, also known as base-2 numeral system. This means that it only talks in 1s and 0s.
     <br>
     In computing terms, we group binary into 8 numbers, or bits. Technically, a bit is a binary digit. You should know that a group of 8 bits is referred to as a byte. So a byte of zeroes and ones could look like 10011011. Each byte can store one character, and we can have 256 possible values, thanks to the base-2 system, 2 to the 8th. In computer talk, this byte could mean something like the letter C.
     </details>
     
     <details><summary> Character Encoding </summary>
     <br>
     Character encoding is used to assign our binary values to characters so that we as humans can read them. You can think of character encoding as a dictionary. It's a way for your computers to look up which human characters should be represented by a given binary value. The oldest character encoding standard used this ASCII. It represents the English alphabet, digits, and punctuation marks. The great thing with ASCII was that we only needed to use 127 values out of our possible 256. It lasted for a very long time, but eventually it wasn't enough. 
     <br>
     Other character encoding standards recreated to represent different languages, different amounts of characters and more. Eventually they would require more than 256 values we were allowed to have. Then came UTF 8. The most prevalent encoding standard used today. Along with having the same ASCII table, it also lets us use a variable number of bytes.
     <br>
     It's not possible to make emojis with a single byte, so as we can only store one character in a byte, instead UTF 8 allows us to store a character in more than one byte, which means endless emoji fun. UTF 8 is built off the Unicode Standard. The Unicode Standard helps us represent character encoding in a consistent manner. 
     <br><br>
     RGB or red, green, and blue model. Just like the actual colors, if you mix a combination of any of these, you'll be able to get the full range of colors. In computerland, we use 3 characters for the RGB model. Each character represents a shade of the color and that then changes the color of the pixel you see on your screen.
     
     <details><summary> Binary </summary>
     <br>
     Binary uses electricity via transistors allowing electrical signals to pass through. There's an electric voltage, we would denote it as one. If there isn't, we would denote it by zero. For just having transistors isn't enough for our computer to be able to do complex tasks. Logic gates allow our transistors to do more complex tasks, like decide where to send electrical signals depending on logical conditions.
     </details>
     
     _Supplemental Reading_ on [Logic Gates](https://simple.wikipedia.org/wiki/Logic_gate)
     
     <details><summary> How to Count in Binary </summary>
     <br>
     (For better understanding, watch [How to count in binary](https://www.youtube.com/watch?v=puaaRoWL-Ec)
     <br><br>
     Binary is the fundamental communication block of computers, but it's used to represent more than just text and images. It's used in many aspects of computing like computer networking. The binary system is how our computers count using ones and zeros, but humans don't count like that. When you were a child, you may have counted using ten fingers on your hand. That innate counting system is called the decimal form or base-10 system. In the decimal system, there are 10 possible numbers you can use ranging from zero to nine. When we count binary, which only uses zero and one, we convert it to a system that we can understand, decimal. 330, 250, 2, 40, 4 million, they're all decimal numbers.
     <br><br>
     Let's consider these numbers: 128, 64, 32, 16, 8, 4, 2, and 1. What patterns do you see? Hopefully, you'll see that each number is a double of the previous number going right to left. What happens if you add them all up? You get 255. That's kind of weird. I thought we could have 256 values for a byte. Well, we do. The zero is counted as a value, so the maximum decimal number you can have is 255.
     </details>
     
- ## **Computer Architecture Layer**

     <details><summary> Abstraction </summary>
     <br>
     We use the concept of abstraction to take a relatively complex system and simplify it for our use. In computing, we use abstraction to make a very complex problem, like how to make computers work, easier to think about. We do that by breaking it apart into simpler ideas that describe single concepts or individual jobs that need to be done, and then stack them in layers. This concept of abstraction will be used throughout this entire course. It's a fundamental concept in the computing world. One other simple example of abstraction in an IT role that you might see a lot is an error message. We don't have to dig through someone else's code and find a bug. This has been abstracted out for us already in the form of an error message. A simple error message like file not found actually tells us a lot of information and saves us time to figure out a solution. Can you imagine if instead of abstracting an error message our computer did nothing and we had no clue where to start looking for answers? Abstraction helps us in many ways that we don't even realize.
 
---

## Week2
     
     
     <br><br><br><br><br><br><br><br><br><br><br>
     # _Full Version Coming Soon_
